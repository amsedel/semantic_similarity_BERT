{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uq8sbfogrQJy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score, jaccard_score, f1_score, homogeneity_completeness_v_measure, adjusted_rand_score\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, pairwise_distances\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08HLAANDw06m",
        "outputId": "05377952-2075-4810-8662-30e49c401696"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Stsbenchmark.tar.gz http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
        "shutil.unpack_archive('./Stsbenchmark.tar.gz', extract_dir='./', format='gztar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4BiyNxQhY4T",
        "outputId": "1b2dcbe6-af33-4518-de87-bc5ab19274eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 05:00:12--  http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
            "Resolving ixa2.si.ehu.es (ixa2.si.ehu.es)... 158.227.106.100\n",
            "Connecting to ixa2.si.ehu.es (ixa2.si.ehu.es)|158.227.106.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://ixa2.si.ehu.eus/stswiki/images/4/48/Stsbenchmark.tar.gz [following]\n",
            "--2024-01-04 05:00:28--  http://ixa2.si.ehu.eus/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
            "Resolving ixa2.si.ehu.eus (ixa2.si.ehu.eus)... 158.227.106.100\n",
            "Connecting to ixa2.si.ehu.eus (ixa2.si.ehu.eus)|158.227.106.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409630 (400K) [application/x-gzip]\n",
            "Saving to: ‘Stsbenchmark.tar.gz’\n",
            "\n",
            "Stsbenchmark.tar.gz 100%[===================>] 400.03K   387KB/s    in 1.0s    \n",
            "\n",
            "2024-01-04 05:01:33 (387 KB/s) - ‘Stsbenchmark.tar.gz’ saved [409630/409630]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getSTSBenchmarkSents(filename='sts-train.csv', root='stsbenchmark/', encoding='utf-8'):\n",
        "  f = open(root+filename, 'r', encoding=encoding)\n",
        "  s1, s2, target = [], [], []\n",
        "  for line in f:\n",
        "    example = re.split(r'\\t+', line)\n",
        "    if len(example) > 7:\n",
        "      example = example[:-2]\n",
        "    s2.append(example[-1])\n",
        "    s1.append(example[-2])\n",
        "    target.append(float(example[-3]))\n",
        "  print(\"{} samples: {}\".format(filename, len(target)))\n",
        "  return s1, s2, target"
      ],
      "metadata": {
        "id": "jJCuyxk3hmEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1_test,s2_test,target_test= getSTSBenchmarkSents(filename='sts-test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EcrzLWThrkg",
        "outputId": "b57a1ef5-2507-4853-9ba8-77f951ac1fcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sts-test.csv samples: 1379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_PATH = \"bert-base-uncased\"\n",
        "root_drive = '/content/drive/MyDrive/Tesis/STS_Benchmark/transformer_tunned_BERT/uncase_base/'"
      ],
      "metadata": {
        "id": "CfBedtrSv9bN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKElhIi44LJD",
        "outputId": "b660adda-5b17-4ee3-8c85-77f7c1069703"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(num_sent, num_capa, num_cabezal)\n",
        "#all_attentions_matrix = torch.load(root_drive + BERT_PATH + '_all_attentions_test_complete.pth')\n",
        "all_attentions_matrix = torch.load(root_drive + BERT_PATH + '_all_attentions_test_STS-B.pth')"
      ],
      "metadata": {
        "id": "tknswpDAwi-j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = 12\n",
        "heads = 12\n",
        "num_sentences = list(all_attentions_matrix.keys())[-1][0]+1\n",
        "attentions_concat_heads = {}\n",
        "attentions_matrix_list = []\n",
        "attentions_list = []\n",
        "\n",
        "for i in range(num_sentences):\n",
        "  for j in range(layers):\n",
        "    tensor_list = []\n",
        "    for k in range(heads):\n",
        "      tensor_list.append(torch.tensor(all_attentions_matrix[(i,j,k)]['vectors']).flatten())\n",
        "    #attentions_concat_heads[(i,j)] = torch.stack(tensor_list).unsqueeze(0).permute(0, 2, 1)\n",
        "    stack = torch.stack(tensor_list)\n",
        "    attentions_concat_heads[(i,j)] = stack\n",
        "    attentions_matrix_list.append(stack)\n",
        "    #attentions_list.append(((i,j),all_attentions_matrix[(i,j,k)]['label'], stack))\n",
        "    attentions_list.append(((i,j),all_attentions_matrix[(i,j,k)]['sequence'],all_attentions_matrix[(i,j,k)]['label'],all_attentions_matrix[(i,j,k)]['dimension'],stack))\n",
        "attentions_list = [(id,s,label,dim,tensor.permute(1,0)) for id, s, label, dim, tensor in attentions_list]"
      ],
      "metadata": {
        "id": "KAexbnf3I632"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attentions_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GKJ74Gwpd-R",
        "outputId": "ae25f13b-6ac8-4107-b3e5-92000c0b4d0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 0),\n",
              " 's1: A girl is styling her hair. s2: A girl is brushing her hair.',\n",
              " 2.5,\n",
              " 17,\n",
              " tensor([[0.0482, 0.7595, 0.7518,  ..., 0.0573, 0.6803, 0.8649],\n",
              "         [0.1027, 0.0157, 0.0206,  ..., 0.0765, 0.0337, 0.0415],\n",
              "         [0.0210, 0.0078, 0.0071,  ..., 0.0599, 0.0072, 0.0022],\n",
              "         ...,\n",
              "         [0.0825, 0.0444, 0.0269,  ..., 0.0435, 0.0954, 0.0390],\n",
              "         [0.1232, 0.3039, 0.3509,  ..., 0.0428, 0.1945, 0.0027],\n",
              "         [0.2252, 0.0845, 0.1228,  ..., 0.0388, 0.3616, 0.2844]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attentions_list[0][4].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXBO0gqRwGuf",
        "outputId": "75539c0d-12ff-410e-85ac-d7e2d696f17e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([289, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "8-t7cRSS-NRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un sampler secuencial\n",
        "# attentions_list = dataset\n",
        "# SequentialSampler does not perform any shuffling or random selection of items.\n",
        "sampler = SequentialSampler(attentions_list)\n",
        "\n",
        "# Definir el tamaño del lote\n",
        "batch_size = 12 # always 12, because it is the number of attention layers, 12 layers for the same sentence\n",
        "\n",
        "# Crear el DataLoader sin un BatchSampler\n",
        "dataloader = DataLoader(attentions_list, batch_size=batch_size, sampler=sampler)"
      ],
      "metadata": {
        "id": "dSzUvRBd7oaW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader) #1379 OK because there are 1379 sentences, 1 batch = 1 same sentence , 12 elements in the batch because there are 12 layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmpqNtb9cNp",
        "outputId": "e70a588a-a92a-48ae-9078-4f41cb4afb5d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1379"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sr = next(iter(dataloader))\n",
        "sr[0] # ids (num_sent, num_layer)\n",
        "sr[1] # similarity label\n",
        "sr[2].shape\n",
        "sr[3].shape\n",
        "sr[4].shape #[batch_size, len_sentence, input_size] with attentions ([12,289,12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RUVAl03NZqO",
        "outputId": "f3baca9f-97ac-4d90-c698-47dc0c625650"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 289, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "lh2_LvEAFOuX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, iterator, optimizer, criterion, device=device, clip = 1.0):\n",
        "    #Training loop\n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    seed = 42\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    #torch.cuda.set_device(0)\n",
        "\n",
        "    for i, (_,_,_,_,input) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input)\n",
        "        loss = criterion(output, input)\n",
        "        loss.backward()\n",
        "        #prevent gradients from exploding\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        #Update params\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "    epoch_train_loss = loss_sum * batch_size / len(iterator)\n",
        "\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "cJcjfZz_-uD3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer el vector latente fijo de cada elemento del batch\n",
        "def extract_latent_vectors(model, dataloader, device, model_type='RNN'):\n",
        "    model.eval()\n",
        "    vector_representations = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (id,s,label,dim,input) in dataloader:\n",
        "            latent_vectors = []\n",
        "            if model_type == 'RNN':\n",
        "              _, (latent_representation, _) = model.encoder(input)\n",
        "              latent = latent_representation.squeeze(0)\n",
        "            else:\n",
        "              latent_representation = model.encoder(input)\n",
        "              latent = latent_representation.squeeze(0).squeeze(1)\n",
        "            tuples = list(zip(id[0].tolist(), id[1].tolist()))\n",
        "            #latent = latent_representation.squeeze(0)\n",
        "            for i in range(latent.size(0)):\n",
        "              latent_vectors.append(latent[i].numpy())\n",
        "              vector_representations[tuples[i]] = { 'vector' : latent[i].numpy(), 'sequence': s, 'label': label, 'dimension':dim}\n",
        "\n",
        "    return vector_representations"
      ],
      "metadata": {
        "id": "hZ3fWPpSI-pq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la arquitectura del autoencoder con LSTM\n",
        "class AutoencoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size = 128):\n",
        "        super(AutoencoderLSTM, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        # Codificación\n",
        "        #o = [1, 12, 128] = [batch_size, len_sents, hidden_size]\n",
        "        #x = [1, 12, 289] = [batch_size, len_sents, input_size]\n",
        "        #print(x.shape)\n",
        "        o, (h_n, _) = self.encoder(x)\n",
        "        #h = [1, 1, 128] = [batch_size, num_layers * num_directions, hidden_size]\n",
        "        # Reducción a tamaño latente\n",
        "        #latent = [1, 128] = [num_layers * num_directions, hidden_size]\n",
        "        latent = h_n.squeeze(0)\n",
        "        # Decodificación\n",
        "        #output, _ = self.decoder(latent.unsqueeze(0).repeat(1, x.size(1), 1))\n",
        "        output, _ = self.decoder(o)\n",
        "        return output, latent\n"
      ],
      "metadata": {
        "id": "CspMdXG6P9dc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 4\n",
        "best_valid_loss = float('inf')\n",
        "model_name = 'Autoencoder_LSTM'\n",
        "train_loss_values = []\n",
        "history = {\"train\": {\"loss\": []}}\n",
        "\n",
        "input_size = 12  #due to 768 dimensión of BERT\n",
        "hidden_size = 2  # size of fixed vector #laten dim\n",
        "learning_rate = 0.001 #0.00025 #0.0007\n",
        "\n",
        "seed = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "model = AutoencoderLSTM(input_size, hidden_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    epoch_train_loss = train_loop(model,dataloader,optimizer,criterion,device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "\n",
        "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
        "\n",
        "    print('-' * 80)\n",
        "    #print(f'Epoch: {epoch+1:03}/{NUM_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train loss: {epoch_train_loss:.4f} | Train acc: {epoch_train_acc:.4f} | Dev loss: {epoch_dev_loss:.4f} | Dev acc: {epoch_dev_acc:.4f}')\n",
        "    print(f'Epoch: {epoch+1:03}/{NUM_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train loss: {epoch_train_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "1VTc_w-0EI_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef6807d-10e8-400d-f1f4-9294fad43ee9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Epoch: 001/4 | Epoch Time: 0m 45s | Train loss: 0.0884\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 002/4 | Epoch Time: 0m 45s | Train loss: 0.0511\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 003/4 | Epoch Time: 0m 45s | Train loss: 0.0472\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 004/4 | Epoch Time: 0m 44s | Train loss: 0.0466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_representations = extract_latent_vectors(model, dataloader, device)"
      ],
      "metadata": {
        "id": "rmqDO8T-doKJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_representations[(0,0)]['vector']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "387PV-bDzyzd",
        "outputId": "8e1e5e04-b9ec-4eb2-f5ca-4e69bc51dfe8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.08917949, -0.19299325], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el diccionario en un archivo\n",
        "torch.save(vector_representations, root_drive + BERT_PATH + str(hidden_size) + '_vector_representations_ATTENTIONS_2D.pth')"
      ],
      "metadata": {
        "id": "hAkzCvQ8p4OQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 2  # size of fixed vector\n",
        "vector_representations = torch.load(root_drive + BERT_PATH + str(hidden_size) + '_vector_representations_ATTENTIONS_2D.pth')\n",
        "# el archivo '_vector_representations_ATTENTIONS_2D.pth' puede ser directamente cargado en la herramienta visual de interpretabilidad opción ()"
      ],
      "metadata": {
        "id": "AmOB1Djot8z0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vector_representations) #(num_sentence,layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32RlZGvQo8XE",
        "outputId": "137928d5-8286-4343-ceef-508eafb7fefb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16548"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_representations_per_layer(num_sentences, vector_representations, layers = 12):\n",
        "  vectors_per_layer = {}\n",
        "  labels = {}\n",
        "  for l in range(layers):\n",
        "    #vectors_per_layer[l] = np.array([vector_representations[(i,l)]['vector'].detach().numpy() for i in range(num_sentences)])\n",
        "    vectors_per_layer[l] = np.array([vector_representations[(i,l)]['vector'] for i in range(num_sentences)])\n",
        "  labels = { i: vector_representations[(i,0)]['label'][0].item() for i in range(num_sentences)}\n",
        "  sequences = { i: vector_representations[(i,0)]['sequence'][0] for i in range(num_sentences)}\n",
        "  dimensions = { i: vector_representations[(i,0)]['dimension'][0].item() for i in range(num_sentences)}\n",
        "  return vectors_per_layer, sequences, labels, dimensions"
      ],
      "metadata": {
        "id": "XQ-dXKeYTHok"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_per_layer, sequences, labels, dimensions = get_representations_per_layer(num_sentences, vector_representations)"
      ],
      "metadata": {
        "id": "mBvM7amZVhFB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el diccionario en un archivo\n",
        "torch.save(vectors_per_layer, root_drive + BERT_PATH + str(hidden_size) + '_vectors_per_layer_ATT_LSTM_prueba.pth')"
      ],
      "metadata": {
        "id": "iZJHKzT9CB5x"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}