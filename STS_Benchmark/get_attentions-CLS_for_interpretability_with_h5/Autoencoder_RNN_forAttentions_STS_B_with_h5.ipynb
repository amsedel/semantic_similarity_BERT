{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uq8sbfogrQJy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score, jaccard_score, f1_score, homogeneity_completeness_v_measure, adjusted_rand_score\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, pairwise_distances\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import torch.nn.functional as F\n",
        "import h5py\n",
        "import ast\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08HLAANDw06m",
        "outputId": "7dcb31e5-d6dc-4787-ae26-c9848ce7f728"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Stsbenchmark.tar.gz http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
        "shutil.unpack_archive('./Stsbenchmark.tar.gz', extract_dir='./', format='gztar')"
      ],
      "metadata": {
        "id": "7mJJva7EcnjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64416a2c-4c1b-4b4e-c60b-1803055358ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-26 04:42:55--  http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
            "Resolving ixa2.si.ehu.es (ixa2.si.ehu.es)... 158.227.106.100\n",
            "Connecting to ixa2.si.ehu.es (ixa2.si.ehu.es)|158.227.106.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://ixa2.si.ehu.eus/stswiki/images/4/48/Stsbenchmark.tar.gz [following]\n",
            "--2024-01-26 04:42:55--  http://ixa2.si.ehu.eus/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
            "Resolving ixa2.si.ehu.eus (ixa2.si.ehu.eus)... 158.227.106.100\n",
            "Connecting to ixa2.si.ehu.eus (ixa2.si.ehu.eus)|158.227.106.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409630 (400K) [application/x-gzip]\n",
            "Saving to: ‘Stsbenchmark.tar.gz’\n",
            "\n",
            "Stsbenchmark.tar.gz 100%[===================>] 400.03K   465KB/s    in 0.9s    \n",
            "\n",
            "2024-01-26 04:42:56 (465 KB/s) - ‘Stsbenchmark.tar.gz’ saved [409630/409630]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getSTSBenchmarkSents(filename='sts-train.csv', root='stsbenchmark/', encoding='utf-8'):\n",
        "  f = open(root+filename, 'r', encoding=encoding)\n",
        "  s1, s2, target = [], [], []\n",
        "  for line in f:\n",
        "    example = re.split(r'\\t+', line)\n",
        "    if len(example) > 7:\n",
        "      example = example[:-2]\n",
        "    s2.append(example[-1])\n",
        "    s1.append(example[-2])\n",
        "    target.append(float(example[-3]))\n",
        "  print(\"{} samples: {}\".format(filename, len(target)))\n",
        "  return s1, s2, target"
      ],
      "metadata": {
        "id": "jJCuyxk3hmEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1_test,s2_test,target_test= getSTSBenchmarkSents(filename='sts-test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EcrzLWThrkg",
        "outputId": "53380bfb-6581-4929-eca1-b5cc7ccab4f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sts-test.csv samples: 1379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_PATH = \"bert-base-uncased\"\n",
        "root_drive = '/content/drive/MyDrive/Tesis/STS_Benchmark/transformer_tunned_BERT/uncase_base/'"
      ],
      "metadata": {
        "id": "CfBedtrSv9bN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKElhIi44LJD",
        "outputId": "9f43512d-bb6c-4f81-ac20-bf4aba94bd6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_with_hdf5(file_path, data_dict):\n",
        "    # Save data in HDF5 format\n",
        "    with h5py.File(file_path, 'w') as hf:\n",
        "        # Iterate through the dictionary and save the data to the HDF5 file\n",
        "        def save_dict(group, data):\n",
        "            for key, value in data.items():\n",
        "                key_str = str(key)  # Convert the key to a string\n",
        "                if isinstance(value, dict):\n",
        "                    # If the value is a dictionary, create a new group and save the data inside it\n",
        "                    subgroup = group.create_group(key_str)\n",
        "                    save_dict(subgroup, value)\n",
        "                elif isinstance(value, np.ndarray):\n",
        "                    # If the value is a NumPy array, save it as a dataset\n",
        "                    group.create_dataset(key_str, data=value)\n",
        "                else:\n",
        "                    # For other data types, save them as attributes\n",
        "                    group.attrs[key_str] = str(value)  # Convert the value to a string and save it as an attribute\n",
        "\n",
        "        save_dict(hf, data_dict)"
      ],
      "metadata": {
        "id": "vwILPcnLp7H0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_tuple(string):\n",
        "    try:\n",
        "        result = ast.literal_eval(string)\n",
        "        return isinstance(result, tuple)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return False"
      ],
      "metadata": {
        "id": "-HNxqbJcYs7_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_with_hdf5(file_path):\n",
        "    loaded_data = {}\n",
        "\n",
        "    # Load data from the HDF5 file\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        # Recursive function to load data from the HDF5 file\n",
        "        def load_dict(group):\n",
        "            result = {}\n",
        "            for key, item in group.items():\n",
        "                # Use the key directly if it is a string, otherwise, convert back to tuple\n",
        "                key_obj = ast.literal_eval(key) if is_tuple(key) else key\n",
        "                if isinstance(item, h5py.Group):\n",
        "                    # If it's a group, load its data recursively\n",
        "                    result[key_obj] = load_dict(item)\n",
        "                    # Retrieve attributes from the group\n",
        "                    for attr_key, attr_value in item.attrs.items():\n",
        "                        result[key_obj][attr_key] = attr_value\n",
        "                else:\n",
        "                    # If it's a dataset, load its values\n",
        "                    result[key_obj] = torch.tensor(np.array(item))\n",
        "            return result\n",
        "\n",
        "        loaded_data = load_dict(hf)\n",
        "\n",
        "    return loaded_data"
      ],
      "metadata": {
        "id": "1dWJRmZJYrfF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'embeddings_att_sts_b.h5'"
      ],
      "metadata": {
        "id": "uX58jKR0Y6hF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp {root_drive + file_name} /content/"
      ],
      "metadata": {
        "id": "Wt0bSVTKZ0Kt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_attentions_matrix = load_with_hdf5(root_drive + file_name)\n",
        "all_attentions_matrix = load_with_hdf5(file_name)"
      ],
      "metadata": {
        "id": "tknswpDAwi-j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_attentions_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm9exZEsB7sh",
        "outputId": "4d0e9e03-754b-4fb3-f3ac-3211cd0d1825"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198576"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = 12\n",
        "heads = 12\n",
        "#num_sentences = list(all_attentions_matrix.keys())[-1][0]+1\n",
        "num_sentences = int(len(all_attentions_matrix.keys())/(layers*heads))\n",
        "attentions_concat_heads = {}\n",
        "attentions_matrix_list = []\n",
        "attentions_list = []\n",
        "\n",
        "for i in range(num_sentences):\n",
        "  for j in range(layers):\n",
        "    tensor_list = []\n",
        "    for k in range(heads):\n",
        "      tensor_list.append(torch.tensor(all_attentions_matrix[(i,j,k)]['vectors']).flatten())\n",
        "    #attentions_concat_heads[(i,j)] = torch.stack(tensor_list).unsqueeze(0).permute(0, 2, 1)\n",
        "    stack = torch.stack(tensor_list)\n",
        "    attentions_concat_heads[(i,j)] = stack\n",
        "    attentions_matrix_list.append(stack)\n",
        "    #attentions_list.append(((i,j),all_attentions_matrix[(i,j,k)]['label'], stack))\n",
        "    attentions_list.append(((i,j),all_attentions_matrix[(i,j,k)]['sequence'],all_attentions_matrix[(i,j,k)]['label'],all_attentions_matrix[(i,j,k)]['dimension'],stack))\n",
        "attentions_list = [(id,s,label,dim,tensor.permute(1,0)) for id, s, label, dim, tensor in attentions_list]"
      ],
      "metadata": {
        "id": "KAexbnf3I632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5315c782-ec0e-4ba2-b9dd-0481c1ba65b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-56402357c72e>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_list.append(torch.tensor(all_attentions_matrix[(i,j,k)]['vectors']).flatten())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(attentions_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGj5N56IdE5R",
        "outputId": "9dce4039-5de1-45f0-db4d-7613e03116ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16548"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attentions_list[24]"
      ],
      "metadata": {
        "id": "3GKJ74Gwpd-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a494325f-a7b9-421f-a5f5-9fe2ea78c20a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 0),\n",
              " \"s1: One woman is measuring another woman's ankle. s2: A woman measures another woman's ankle.\",\n",
              " '5.0',\n",
              " '22',\n",
              " tensor([[0.0408, 0.6959, 0.4869,  ..., 0.0665, 0.6562, 0.8600],\n",
              "         [0.0776, 0.0043, 0.0227,  ..., 0.1529, 0.0165, 0.0114],\n",
              "         [0.0221, 0.0020, 0.0099,  ..., 0.0690, 0.0122, 0.0037],\n",
              "         ...,\n",
              "         [0.0360, 0.0218, 0.0171,  ..., 0.0340, 0.0367, 0.0661],\n",
              "         [0.1160, 0.4015, 0.3665,  ..., 0.0342, 0.1426, 0.0011],\n",
              "         [0.1761, 0.0575, 0.1498,  ..., 0.0353, 0.5425, 0.1911]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attentions_list[0][4].shape"
      ],
      "metadata": {
        "id": "oXBO0gqRwGuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f0caba-9fdc-4dcb-cbc2-c3ab5353801d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([289, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "8-t7cRSS-NRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un sampler secuencial\n",
        "# attentions_list = dataset\n",
        "# SequentialSampler does not perform any shuffling or random selection of items.\n",
        "sampler = SequentialSampler(attentions_list)\n",
        "\n",
        "# Definir el tamaño del lote\n",
        "batch_size = 12 # always 12, because it is the number of attention layers, 12 layers for the same sentence\n",
        "\n",
        "# Crear el DataLoader sin un BatchSampler\n",
        "dataloader = DataLoader(attentions_list, batch_size=batch_size, sampler=sampler)"
      ],
      "metadata": {
        "id": "dSzUvRBd7oaW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader) #1379 OK because there are 1379 sentences, 1 batch = 1 same sentence , 12 elements in the batch because there are 12 layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmpqNtb9cNp",
        "outputId": "79b14e97-3ae1-4e31-b1cd-69f4dcf29321"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1379"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sr = next(iter(dataloader))\n",
        "sr[0] # ids (num_sent, num_layer)\n",
        "sr[1] # sequence\n",
        "sr[2] # similarity label\n",
        "sr[3] # dimension\n",
        "sr[4].shape #[batch_size, len_sentence, input_size] with attentions ([12,289,12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RUVAl03NZqO",
        "outputId": "644b51b4-44c4-45b9-b470-84aa65c02669"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 289, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "lh2_LvEAFOuX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, iterator, optimizer, criterion, device=device, clip = 1.0):\n",
        "    #Training loop\n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    seed = 42\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    #torch.cuda.set_device(0)\n",
        "\n",
        "    for i, (_,_,_,_,input) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input)\n",
        "        loss = criterion(output, input)\n",
        "        loss.backward()\n",
        "        #prevent gradients from exploding\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        #Update params\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "    epoch_train_loss = loss_sum * batch_size / len(iterator)\n",
        "\n",
        "    return epoch_train_loss"
      ],
      "metadata": {
        "id": "cJcjfZz_-uD3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer el vector latente fijo de cada elemento del batch\n",
        "def extract_latent_vectors(model, dataloader, device, model_type='RNN'):\n",
        "    model.eval()\n",
        "    vector_representations = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (id,s,label,dim,input) in dataloader:\n",
        "            latent_vectors = []\n",
        "            if model_type == 'RNN':\n",
        "              _, (latent_representation, _) = model.encoder(input)\n",
        "              latent = latent_representation.squeeze(0)\n",
        "            else:\n",
        "              latent_representation = model.encoder(input)\n",
        "              latent = latent_representation.squeeze(0).squeeze(1)\n",
        "            tuples = list(zip(id[0].tolist(), id[1].tolist()))\n",
        "            #latent = latent_representation.squeeze(0)\n",
        "            for i in range(latent.size(0)):\n",
        "              latent_vectors.append(latent[i].numpy())\n",
        "              vector_representations[tuples[i]] = { 'vector' : latent[i].numpy(), 'sequence': s, 'label': label, 'dimension':dim}\n",
        "\n",
        "    return vector_representations"
      ],
      "metadata": {
        "id": "hZ3fWPpSI-pq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la arquitectura del autoencoder con LSTM\n",
        "class AutoencoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size = 128):\n",
        "        super(AutoencoderLSTM, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_size, input_size, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        # Codificación\n",
        "        #o = [1, 12, 128] = [batch_size, len_sents, hidden_size]\n",
        "        #x = [1, 12, 289] = [batch_size, len_sents, input_size]\n",
        "        #print(x.shape)\n",
        "        o, (h_n, _) = self.encoder(x)\n",
        "        #h = [1, 1, 128] = [batch_size, num_layers * num_directions, hidden_size]\n",
        "        # Reducción a tamaño latente\n",
        "        #latent = [1, 128] = [num_layers * num_directions, hidden_size]\n",
        "        latent = h_n.squeeze(0)\n",
        "        # Decodificación\n",
        "        #output, _ = self.decoder(latent.unsqueeze(0).repeat(1, x.size(1), 1))\n",
        "        output, _ = self.decoder(o)\n",
        "        return output, latent\n"
      ],
      "metadata": {
        "id": "CspMdXG6P9dc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 4\n",
        "best_valid_loss = float('inf')\n",
        "model_name = 'Autoencoder_LSTM'\n",
        "train_loss_values = []\n",
        "history = {\"train\": {\"loss\": []}}\n",
        "\n",
        "input_size = 12  #due to 768 dimensión of BERT\n",
        "hidden_size = 2  # size of fixed vector #laten dim\n",
        "learning_rate = 0.001 #0.00025 #0.0007\n",
        "\n",
        "seed = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "model = AutoencoderLSTM(input_size, hidden_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    epoch_train_loss = train_loop(model,dataloader,optimizer,criterion,device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "\n",
        "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
        "\n",
        "    print('-' * 80)\n",
        "    #print(f'Epoch: {epoch+1:03}/{NUM_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train loss: {epoch_train_loss:.4f} | Train acc: {epoch_train_acc:.4f} | Dev loss: {epoch_dev_loss:.4f} | Dev acc: {epoch_dev_acc:.4f}')\n",
        "    print(f'Epoch: {epoch+1:03}/{NUM_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train loss: {epoch_train_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "1VTc_w-0EI_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b97d217-380b-4b92-ed65-450cf1b6ba14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Epoch: 001/4 | Epoch Time: 1m 10s | Train loss: 0.0875\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 002/4 | Epoch Time: 1m 9s | Train loss: 0.0503\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 003/4 | Epoch Time: 1m 13s | Train loss: 0.0462\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch: 004/4 | Epoch Time: 1m 13s | Train loss: 0.0452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_representations = extract_latent_vectors(model, dataloader, device)"
      ],
      "metadata": {
        "id": "rmqDO8T-doKJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_representations[(0,0)]['vector']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "387PV-bDzyzd",
        "outputId": "b33284df-d38a-420a-e989-ec85f5df0ee1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.29809073, -0.13135618], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_with_hdf5('reduced_vectors_sts_b_att_2D.h5', vector_representations)"
      ],
      "metadata": {
        "id": "tYaMFe_AqDKZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BdlJHLZVrvOQ",
        "outputId": "5c547fa8-51aa-41d5-f4a4-bd193a4afe0e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Tesis/STS_Benchmark/transformer_tunned_BERT/uncase_base/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp 'reduced_vectors_sts_b_att_2D.h5' {root_drive}"
      ],
      "metadata": {
        "id": "kHNIz9gCrN2L"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vector_representations) #(num_sentence,layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32RlZGvQo8XE",
        "outputId": "cca892b5-24d0-4e8c-fb35-919ecdc022e9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16548"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save with PyTorch\n",
        "#torch.save(vector_representations, 'reduced_vectors_sts_b_att_2D.pth')"
      ],
      "metadata": {
        "id": "AGjjfFzfKWSo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load with PyTorch\n",
        "#data_loaded = torch.load('reduced_vectors_sts_b_att_2D.pth')"
      ],
      "metadata": {
        "id": "BzyWqXJqK2R4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_num(text):\n",
        "    try:\n",
        "        # Try to convert the text to a number (int or float)\n",
        "        number = float(text)\n",
        "        # Check if the number is an integer, and if so, convert it to int\n",
        "        if number.is_integer():\n",
        "            return int(number)\n",
        "        return number\n",
        "    except ValueError:\n",
        "        # If there is a ValueError, return the original text\n",
        "        return text"
      ],
      "metadata": {
        "id": "MmjHPaj5w5_m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_representations_per_layer(num_sentences, vector_representations, layers = 12, main_key = 'vector', add_keys = ['sequence','label','dimension']):\n",
        "  vectors_per_layer = {}\n",
        "  labels = {}\n",
        "  data = []\n",
        "  for l in range(layers):\n",
        "    vectors_per_layer[l] = np.array([vector_representations[(i,l)][main_key] for i in range(num_sentences)])\n",
        "  data.append(vectors_per_layer)\n",
        "  for j in add_keys:\n",
        "    data.append({i: vector_representations[(i, 0)][j][0].item() if isinstance(vector_representations[(i, 0)][j][0], torch.Tensor) else convert_to_num(vector_representations[(i, 0)][j][0]) for i in range(num_sentences)})\n",
        "  return tuple(data)"
      ],
      "metadata": {
        "id": "EfSgYSnaoq_v"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_representations[(0,0)]['label'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jBjE7TBjskDP",
        "outputId": "7d2931dd-464c-4cb0-aac5-673541a78faa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_per_layer, sequences, labels, dimensions = get_representations_per_layer(num_sentences, vector_representations)"
      ],
      "metadata": {
        "id": "mBvM7amZVhFB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_with_hdf5('vectors_per_layer_ATT_LSTM_sts_b_h5.h5', vectors_per_layer)"
      ],
      "metadata": {
        "id": "4GnzEYA7yURV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp 'vectors_per_layer_ATT_LSTM_sts_b_h5.h5' {root_drive}"
      ],
      "metadata": {
        "id": "PQo3Bd7dy8Ka"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}